# ğŸ›¡ï¸ neural-networks-under-attack  
_Training Robust Neural Networks against Adversarial Attacks_

## ğŸ§  Overview

This project investigates the robustness of Convolutional Neural Networks (CNNs) against adversarial attacks using the CIFAR-10 dataset. We implement and evaluate different adversarial attack techniques (FGSM, PGD, DeepFool) and defense strategies (Adversarial Training, Randomized Networks) in two key stages:

1. **Stage 1** â€“ Establish a baseline CNN and test it under adversarial conditions.
2. **Stage 2** â€“ Experiment with advanced attack and defense mechanisms to improve model robustness.

ğŸ“š **Authors**: Lyna Bouikni & Arthur Mussard  
ğŸ« UniversitÃ© Paris Dauphine â€“ Master IASD  
ğŸ“… December 2023

---

## ğŸ¯ Objectives

- âœ… Build and evaluate baseline CNN classifiers (LeNet-style and advanced architecture)
- âœ… Implement adversarial attacks: FGSM, PGD, DeepFool
- âœ… Apply adversarial training to increase model robustness
- âœ… Test randomized networks as a defense strategy
- âœ… Compare performance degradation and recovery under adversarial stress

---

## ğŸ§ª Dataset

- **Name**: [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)
- **Description**: 60,000 32x32 RGB images in 10 classes
- **Usage**: Train/test image classifiers under adversarial conditions

---

## ğŸ“ Repository Structure

```bash
â”œâ”€â”€ models/ # Saved model architectures and weights
â”œâ”€â”€ Stage1.ipynb # Baseline CNN training and FGSM/PGD attacks
â”œâ”€â”€ Stage2.ipynb # Adversarial training, DeepFool, Randomized networks
â”œâ”€â”€ model.py # CNN architecture definition
â”œâ”€â”€ test_project.py # Testing entry point
â”œâ”€â”€ requirements.txt # Required Python packages
â”œâ”€â”€ Slides_Data_Lab_3 (2).pdf # Slide presentation
â”œâ”€â”€ report.pdf # Final report
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
