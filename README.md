# neural-networks-under-attack

# ğŸ›¡ï¸ Training Robust Neural Networks against Adversarial Attacks

## ğŸ§  Overview

This project investigates the robustness of Convolutional Neural Networks (CNNs) against adversarial attacks using the CIFAR-10 dataset. We implement and evaluate different adversarial attack techniques (FGSM, PGD, DeepFool) and defense strategies (Adversarial Training, Randomized Networks) in two key stages:

1. **Establish a baseline classifier and test its vulnerabilities** under adversarial conditions.
2. **Experiment with advanced attack and defense strategies** to improve model robustness and resilience.

ğŸ“š **Authors**: [Lyna Bouikni](https://www.linkedin.com/in/your-profile) & Arthur Mussard  
ğŸ« **University**: Paris Dauphine â€“ M2 IASD  
ğŸ“… **Date**: December 2023

---

## ğŸ¯ Objectives

- âœ… Train and evaluate baseline CNN classifiers (LeNet-style and enhanced architectures)
- âœ… Implement adversarial attacks: FGSM, PGD, DeepFool
- âœ… Apply adversarial training to enhance model robustness
- âœ… Explore randomized networks as a novel defense mechanism
- âœ… Assess performance degradation and recovery under attack scenarios

---

## ğŸ§ª Dataset

- **Name**: [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)
- **Description**: 60,000 32x32 color images across 10 classes
- **Use**: Training and testing adversarial attacks and defenses on image classification tasks

---

## ğŸ—ï¸ Model Architectures

| Model | Description | Accuracy |
|-------|-------------|----------|
| **Model A** | Basic CNN (LeNet-5 style) | 40% |
| **Model B** | Conv + BN + MaxPool x3 + FC x2 | 69% |

- Batch Normalization significantly improved performance in Model B.
- Model B was used for most experiments.

---

## âš”ï¸ Attack Methods

### ğŸ”¸ Fast Gradient Sign Method (FGSM)

- One-step attack that perturbs the input in the direction of the loss gradient.
- Accuracy dropped from 52% â†’ ~35% as epsilon increased.

### ğŸ”¸ Projected Gradient Descent (PGD)

- Multi-step iterative variant of FGSM.
- Stronger than FGSM: accuracy dropped to ~10% at higher perturbation levels.

### ğŸ”¸ DeepFool Attack

- Iteratively modifies inputs to cross decision boundaries with minimal changes.
- Nearly imperceptible perturbations cause significant classification errors.

---

## ğŸ›¡ï¸ Defense Techniques

### ğŸ”¹ Adversarial Training

- Training with adversarial examples improves robustness.
- Both FGSM and PGD adversarial training lead to over 50% accuracy under attacks.
- FGSM adversarial training proved more effective overall.

### ğŸ”¹ Randomized Networks

- Introduced stochastic behavior using dropout at inference time.
- Results:
  - Accuracy on clean data: **11.57%**
  - Accuracy on perturbed (DeepFool) data: **25.07%**
- Suggests moderate gain in robustness through randomness + adversarial exposure.

---

## ğŸ“Š Results Summary

| Method | Accuracy (clean) | Accuracy (under attack) |
|--------|------------------|--------------------------|
| Model A | 40% | Very low under attack |
| Model B | 69% | Drops under FGSM/PGD/DeepFool |
| Adversarially Trained (FGSM) | ~55-60% | Higher stability across attacks |
| Randomized Network (no adv. training) | 11.57% | â€” |
| Randomized + DeepFool training | â€” | 25.07% |

---

## ğŸ§ª Experiments & Code

Main components:
- `cnn_baseline.py`: Base CNN models (Model A & B)
- `attacks.py`: FGSM, PGD, DeepFool implementations
- `train_adv.py`: Adversarial training loops
- `randomized_network.py`: Dropout-enhanced CNN architecture

---

## ğŸ—£ï¸ Key Insights

- Even small perturbations can mislead standard classifiers.
- Adversarial training meaningfully boosts model robustness.
- DeepFool is particularly effective at minimalistic attacks.
- Randomized inference adds stochastic defense but benefits further from adversarial exposure.
- There's a trade-off between model complexity, training time, and robustness.

---

## ğŸ§­ Future Work

- ğŸ”¬ Explore black-box adversarial attacks and defenses.
- ğŸ§  Integrate SHAP/LIME for interpretability under attack.
- âš™ï¸ Deploy models with robust inference pipelines.
- ğŸ“¦ Package robust models for real-time applications (e.g., healthcare, autonomous driving).

---

## ğŸ“¬ Contact

If you'd like to connect or ask questions:

**Lyna Bouikni**  
ğŸ“§ lynabouiknia@.com  
ğŸ”— [LinkedIn]([https://www.linkedin.com/in/your-profile](https://www.linkedin.com/in/lyna-b-231a41126/))

---

## ğŸ“š References

- Goodfellow et al. (2014) â€“ Explaining and Harnessing Adversarial Examples  
- Moosavi-Dezfooli et al. (2016) â€“ DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks  
- Madry et al. (2018) â€“ Towards Deep Learning Models Resistant to Adversarial Attacks

---

_This project was completed as part of the Master IASD program at UniversitÃ© Paris Dauphine._
